---
title: "Power Analyses in Psychophysics"
author: "Björn Jörges"
date: "1/31/2020"
output: html_document
---

***Introduction***

The so-called reproducibility crisis has shaken Psychology to the core. Many effect that the scientific community had deemed established, could not be reproduced in highly powered replication studies. Among these are, prominently, Ego Depletion (@Hagger2016, but see also @Dang) and Terror Management Theory (@Klein2019). @Klein2016 tested a whole series of effects, some of which were not reproduced at all, and some of which were reproduced in terms of statistical significance, but with smaller effect sizes. Different reasons for the lack of reproducibility have been suggested: wide-spread p-hacking, Hypothesizing After Results are Known ("HARKing"), publication bias, lacking power and lacking theory.

<<<<<<< HEAD
Interestingly, Cognitive Psychology, and more specifically Cognitive Psychology that relies on psychophysical methods, has generated less attention in terms of the reproducibility of its results. @Hesse1986 compared different psychophysical methods along criteria such as threshold estimates, efficiency (that is, how many trials it takes to achieve reliable results) and their intra-subject reproducibility in the auditory domain. The reproducibility in the psychophysical measurement of pain sensations has received some attention (@Rosier, @Nilsson).


**Statistical Specifics of Psychophysics**

Prima facie, the study of perception is a methodological outlier in several aspects: the number of subjects tested in a typical experiment is quite low, starting from two or three in older papers. And even the typical sample size in modern studies is rarely much higher than 10. On the other hand, each subject typically performs large numbers of trials, starting from around 50 per condition up to several hundred. Furthermore, psychophysical studies are much more likely to be between-subject designs, which lowers the random variability in responses, thus raising power.


**Different Approaches to Null Hypothesis Testing in Psychophysics**

Interestingly, psychophysics  is one of the few areas of psychology where  Furthermore, there are different established ways of hypo results. Classically, pychometric functions (Cummulative Gaussian or Weibull functions) are fitted for each condition and participant to obtain the Points of Subjective Equality (PSEs) and Just Noticeable Differences (JNDs). This yields one data point per subject and condition, over which a t.test or an ANOVA is performed to test for statistical significance between conditions. This approach neglects that each PSE and JND is based on a large number of trials and thus fails to account for the added reliability of the measures provided. Depending on the experimental design, this approach sacrifices vast amounts of statistical power. As a solution, @Moscatelli2012 have suggested the use of General Linear Mixed Modelling (GLMM). GLMM allows to fit population parameters across all data, while still taking into account that responses within each condition and participant are correlated more strongly than across conditions and participants. In the following, we will estimate power for both types of analyses, to quantify how much power is lost with the two-level approach.


=======
Interestingly, Cognitive Psychology, and more specifically Cognitive Psychology that relies on psychophysical methods, has generated less attention in terms of the reproducibility of its results. @Hesse1986 compared different psychophysical methods along criteria such as threshold estimates, efficiency (that is, how many trials it takes to achieve reliable results) and their reproducibility in the auditory domain. Furthermore, the reproducibility in the psychophysical measurement of pain sensations has received some attention (@Rosier, @Nilsson).

(What do we mean by psychophysics ...)

Prima facie, the study of perception is a methodological outlier in several aspects: the number of subjects tested in a typical experiment is quite low, starting from two or three in older papers. And even the typical sample size in modern studies is rarely much higher than 10. On the other hand, each subject typically performs large numbers of trials, starting from around 50 per condition up to several hundred. Furthermore, there are different established ways of analyzing results. Classically, pychometric functions (Cummulative Gaussian or Weibull functions) are fitted for each condition and participant to obtain the Points of Subjective Equality (PSEs) and Just Noticeable Differences (JNDs). This yields one data point per subject and condition, over which a t-test or an ANOVA are performed to test for statistically significant differences between conditions. 

This approach neglects that each PSE and JND is based on a large number of trials, which leads to a loss of statistical power. @Moscatelli2012 have suggested the use of General Linear Mixed Modelling (GLMM). GLMM allows to obtain population-wide parameters for PSEs and JNDs, while still accounting for inter-subject variability in responses. Mixed Modelling is a more flexible form of linear regression. It allows to fit regression coefficients across the whole population for some parameters, while allowing the coefficients for other parameters to vary within subgroups of the dataset. A classic example is the modelling of the efficacy of a learning intervention on population of students from different classes in one school. Mixed models can account for inherent performance differences between classes, but fit a populationwide coefficient for the efficacy of the intervention. General Linear Mixed Modelling extends this principle by allowing to fit not only linear regression lines, but also other functions – such as cummulative Gaussians, which are commonly used as approximations for psychometric functions. You can find a more thorough explanation and examples in @Moscatelli2012.

One of improvements that have been demanded in the wake of the reproducibility crisis is a more thorough and meticulous study planning. Researchers need to be more aware of underlying theoretical considerations, specify hypotheses before analyzing the data, make precise predictions of how their hypotheses should manifest in their data and formulate statistical models to test these hypotheses. One important step in this process is to make sure that the experiment has sufficient statistical power to detect the postulated effects. We will first show that many experiments using the two-level approach of hypothesis testing described above often lacks power and indicate how much power could be gained for the same experimental designs by using @Moscatelli2002's GLMM approach. In the second part of this paper, we will demonstrate how to plan the sample size in a psychophysical experiment, levering the advantages of the GLMM approach.


***A power comparison between Two-Level approach and GLMM approach***
In the following we will provide an example of how to compute the power in a typical psychophysical experiment with the two different methods described above: The Two-Level approach and the GLMM approach. We then compare the power that each approach provides to detect PSE and JND differences.


```{r setup, include=FALSE}
###Pull the whole repository
require(dplyr)
require(tidyverse)
require(lme4)
require(ggplot2)
require(cowplot)
theme_set(theme_cowplot())

binomial_smooth <- function(...) {
  geom_smooth(method = "glm", method.args = list(family = "binomial"), ...)}

source("Utilities/parabolic.r")
source("Utilities/functions.r")
source("Utilities/colourschemes.r")
set.seed(912)
```


**Information Needed**

This method requires you to provide estimates of all relevant parameters. Some pertain to the stimuli, some can be taken from the literature, and some have to be guessed (educatedly).

*ID* is a vector containing one ID for each subject we want to simulate.

*ConditionOfInterest* is a vector containing IDs for a binary categorical variable related to the main hypothesis of the experiment. For example: Is there a pictorial background scene?

*StandardValues* is a vector containing values for a categorial variable that serves as comparison stimuli. It can contain one value if you want to determine PSEs/JNDs for only one stimulus intensity, but typically you will have several, e. g. when you want to diversify your stimuli to show that a certain effect is not tied to one specific stimulus strength.

*reps* is a vector containing an ID for each trial, the maximum number being the average number of trials we expect for any given staircase.

*PSE_Difference* is a value that indicates the percentage to which the PSEs differ between test and standard condition. It can be zero if the condition of interest is not expected to influence PSEs.

*JND_Difference* is a value that indicates the percentage to which the JNDs differ between test and standard condition. It can be zero if the condition of interest is not expected to influence JNDs.

*Mean_Standard* is the Mean of the psychometric function expected for the standard condition. In many cases, this is the stimulus strength of the comparison stimulus.

*Multiplicator_SD_Standard* is a multiplicator that transforms Weber Fractions or JNDs from the literature into standard deviations for each comparison stimulus.

*SD_Standard* is the Standard Deviation of the psychometric function expected for the standard condition. This standard deviation is proportional to the relevant Weber fraction and JNDs, which are available in the literature. Further below, we explain the simple link between standard deviations of psychometric functions, Weber fractions and JNDs.

*Type_ResponseFunction* describes the function the stimulus strengths are chosen from by the method. It can take the values "normal", "Cauchy" and "uniform". "Normal" and "Cauchy" are recommended when you are using a staircase procedure, while "uniform" corresponds to methods of constant stimuli. For a comparison between the three options, see further below.

*SD_ResponseFunction* further describes the describes the function the stimulus strengths are chosen from. For normal distributions, this value corresponds to its standard deviation; for Cauchy distributions, this corresponds to its scale; and for uniform distributions, this corresponds to the range of values tested.

```{r Staircase1, include=TRUE,echo=TRUE}
ID = paste0("s",1:5)
ConditionOfInterest = c(0,1)
StandardValues = c(6.6, 8, 10)
reps = seq(1,55,1)
PSE_Difference = 1/8
JND_Difference = 1/3
PSE_Standard = StandardValues
Multiplicator_SD_Standard = 0.108
SD_Standard = StandardValues*Multiplicator_SD_Standard
Type_ResponseFunction = "Cauchy"
SD_ResponseFunction = 0.06
```

Next, we simulate one whole data set based on the above values. We first create a data frame with one row for each trial, that is a total of length(ID) x length(ConditionOfInterst) x length(StandardValues) x length(reps) rows.
```{r Staircase3, include=TRUE,echo=TRUE}
  Psychometric = expand.grid(ID=ID, ConditionOfInterest=ConditionOfInterest, StandardValues=StandardValues, reps = reps)
```
  
Then, we draw multiplicators for PSEs and JNDs per subject, accounting for between-subject differences in biases and precision. Omitting this step amounts to the assumption that thisthe effect of interest is equally strong in each participant.
```{r Staircase4, include=TRUE,echo=TRUE}
  Psychometric = Psychometric %>%
    group_by(ID) %>%#
    mutate(PSE_Factor_ID = rnorm(1,1,0.1), #how much variability is in the means of the psychometric functions between subjects?
           SD_Factor_ID = rnorm(1,1,0.1)) #how much variability is in the standard deviations of the psychometric functions between subjects?
```
 
Next, we simulate means and standard deviations of the psychometric functions for each condition and we add between-subject variability. The following lines take the standard PSEs (PSE_Standard) and standard deviations (SD_Standard) as baseline and add the PSE and JND difference in trials assigned to the test condition.
```{r Staircase5, include=TRUE,echo=TRUE}
  Psychometric = Psychometric %>%
    mutate(
      Mean = (StandardValues + (ConditionOfInterest==1)*StandardValues*PSE_Difference),
      SD = abs(SD_Standard + (ConditionOfInterest==1)*SD_Standard*JND_Difference))
```

Now, we factor the between-subject variability in.
```{r Staircase6, include=TRUE,echo=TRUE}
  Psychometric = Psychometric %>%
    mutate(
      Mean = Mean*PSE_Factor_ID,
      SD = SD*SD_Factor_ID)
```

Next, we draw the stimulus strengths likely to be presented in our experiment. As mentioned above, this varies depending on the way the experiment is controlled. For staircase procedures, the responses are more akin to a normal distribution with relatively low standard deviations or to a Cauchy distribution with a low scale. A good way to determine the most appropriate function would be to plot the presented stimulus strengths for pilot data and compare them to different distributions. For the method of constant stimuli, the responses are typically uniformly distributed across 5 to 9 values around the standard stimulus strength. We then use these multipliers ("staircase_factor") to compute the test stimulus strengths presented in the experiment ("Presented_TestStimulusStrength"). Lastly, we compute the difference between test stimulus and standard stimulus for each trial ("Difference").

```{r Staircase7, include=TRUE,echo=TRUE}
if (Type_ResponseFunction == "normal"){
  
  Psychometric = Psychometric %>%
  mutate(
      staircase_factor = pnorm(length(reps),1,SD_ResponseFunction))
  
} else if (Type_ResponseFunction == "Cauchy"){
  
  Psychometric = Psychometric %>%
  mutate(
      staircase_factor = rcauchy(length(reps),1,SD_ResponseFunction))
  
#} else if (Type_ResponseFunction == "uniform"){
#  
#  Psychometric = Psychometric %>%
#  mutate(
#      staircase_factor = seq(SD_ResponseFunction[1],SD_ResponseFunction[2],(SD_ResponseFunction[2]-SD_ResponseFunc#tion[1]/6)))
      
} else{
  
  print("distribution not valid")

}

Psychometric = Psychometric %>%
  mutate(
      staircase_factor = rcauchy(length(reps),1,SD_ResponseFunction), 
      Presented_TestStimulusStrength = Mean*staircase_factor,
      Difference = Presented_TestStimulusStrength - StandardValues)
```

Then, we compute the probability on each trial to judge the test stimulus intensity higher (e. g. the test stimulus was faster, brighter, longer, ...) by feeding the simulated test stimulus strengths in a cummulative Gaussian with the mean and the standard deviations calculated above. We then use this value ("AnswerProbability") to simulate binary answers ("Answer") by drawing responses from a Bernoulli distribution. 

```{r Staircase8, include=TRUE,echo=TRUE}
Psychometric = Psychometric %>%
  mutate(
      AnswerProbability = pnorm(Presented_TestStimulusStrength,Mean,SD),
      
      ##get binary answers ("Test was stronger" yes/no) from probabilities for each trial
      Answer = as.numeric(rbernoulli(length(AnswerProbability),AnswerProbability))
    )
```

As a next step, we bring the data into the format necessary for the glmer() function: We first remove extreme outliers (e. g. by a simple criterion such as excluding trials in which the difference between test and standard stimulus was higher than half the standard stimulus strength). Then, we compute the number of "Test stimulus intensity was higher" responses for each Condition and difference between test and comparison stimulus strength and the number of total observerations for each condition and difference in intensities. 

```{r Staircase9, include=FALSE,echo=TRUE}
  ###prepare for glmer() - needs sum of YES/Total per stimulus strength and condition
  Psychometric = Psychometric %>%
    filter(abs(Difference) < 0.5*abs(Mean)) %>%
    group_by(ID,ConditionOfInterest,StandardValues,Difference) %>%
    mutate(Yes = sum(Answer==1),
           Total = length(ConditionOfInterest))
```

**Plotting the data**

Now, we can inspect these psychometric functions visually to verify whether the values chosen above give rise to the expected psychometric functions in terms of PSE and slopes. As is usual for psychometric functions, we have the difference in stimulus strength between test and comparison on the x axis, the probability to choose the test stimulus as stronger on the y axis. The Condition of Interest is color-coded in blue, while the baseline condition is color-coded in red.The facets correspond to IDs and different standard stimulus strengths, respectively.

```{r Staircase10, include=TRUE}
ggplot(Psychometric, aes(Difference, Answer, color = as.factor(ConditionOfInterest))) +
  binomial_smooth() +
  facet_grid(StandardValues~ID) +
  geom_vline(xintercept = 0, color = "grey") +
  geom_hline(yintercept = 0.5, color = "grey") +
  xlab("Difference between Comparison and Test") +
  ylab("Probability to choose Test") +
  scale_color_manual(name = "Condition",
                     values = c(Red,BlauUB))
```


**Estimating population parameters of the psychometric functions with the GLMM approach**

*Accuracy*
Next, we establish the statistical models we use to test our hypotheses. Following (Moscatelli 2012), we use Generalized Linear Mixed Models for this purpose. For differences in PSEs in our simulated data set, this model would look like the following:

```{r Staircase11, include=TRUE}
GLMM_Accuracy = glmer(cbind(Yes, Total - Yes) ~ ConditionOfInterest + (Difference  | ID)  + (Difference  | StandardValues),
               family = binomial(link = "probit"), 
               data = Psychometric,
               nAGQ = 0,
               control = glmerControl(optimizer = "nloptwrap"))
```

Note that there are different ways of specifying the model depending on assumptions about the data and the hypotheses. This sample specification:
- Assumes
  - That we are interested in a population-wide estimate of the the impact of the condition of interest on PSEs (fixed effect of Condition of Interest)
  - That we are not interested in its population-wide impact on JNDs (no interaction between "Condition of Interest" and "Difference")
- Allows intercepts and slopes to vary per participant. Intercepts correspond to PSEs, while slopes correspond to JNDs. That is, we acommodate individual differences in sensitivity and accuracy.
- Allows intercepts and slopes to vary per standard value. It thus acommodates that different standard values might lead to differences in PSEs and JNDs. For example, higher standard values should lead to more shallow slopes if Weber Fractions hold for the stimulus type under investigation.

Applying the summary() function to the statistical model yields estimates for the coefficients, along with standard errors. Furthermore, the lmerTest package provides the possibility to compute p values using the Satherthwaite degrees of freedom method. The authors of the lme4 package recommend to rely on coefficients and their standard errors alone to estimate the impact of the Condition of Interest. p values from the lmerTest package should thus be regarded as complimentary rather than essential tool. Nonetheless, we believe that p values for the variables of interest are and appropriate, simple proxy for detection of differences between conditions, that allows a quick-and-dirty judgement of whether the Condition of Interest has a significant impact on a simulated dataset. After loading lmerTest, we can inspect coefficient, standard errors and p values as follows: 
```{r Staircase12, include=TRUE}
require(lmerTest)
summary(GLMM_Accuracy)$coefficients
```

*Precision*

Keep in mind that, in the above model, the coefficient of "Difference" corresponds to the slope of the psychometric function. To capture to what extent a manipulation impacts the slope (as a proxy for precision), one needs to determine how much the slope differs between two conditions. That is, we need to estimate the interaction between "Condition of Interest" and "Difference" in our model. A sample specification is the following:
```{r Staircase13, include=TRUE}
GLMM_Precision = glmer(cbind(Yes, Total - Yes) ~ as.factor(ConditionOfInterest)*Difference + (Difference  | ID) + (Difference  | StandardValues), 
               family = binomial(link = "probit"), 
               data = Psychometric,
               nAGQ = 0,
               control = glmerControl(optimizer = "nloptwrap"))
```

We can again use the summary() function in combination with the lmerTest package to extract coefficients, standard errors and p values:

```{r Staircase13, include=TRUE}
require(lmerTest)
summary(GLMM_Precision)$coefficients
```


**Estimating population parameters of the psychometric functions with the Two-Level approach**

For the Two-Level approach, one would first fit psychometric functions for each condition and participant. Then, one would conduct a t.test or an anova to test whether they are different. While there are different methods to fit psychometric functions that each have their own benefits, we use a direction likelihood maximization method (Prins & Kingdom 2010; Knoblauch & Maloney 2012), implemented in the R package quicksy (Linares 2017). The bootstrap option is used to compute confidence intervals, which allow for statistical comparisons. However, the quickpsy package currently does not include an option to estimate population-wide parameters. We thus deactive the bootstrap option, which speeds up the fitting process significantly.

```{r Staircase14, include=TRUE}
require(quickpsy)

Parameters = quickpsy(Psychometric,Difference,Answer,grouping = .(ID,ConditionOfInterest,StandardValues), bootstrap = "none")$par
```

Then, we extract the parameters and bring the output of quickpsy into the adequate format for ANOVA analysis.

```{r Staircase15, include=TRUE}
Parameters2 = Parameters %>%
  filter(parn == "p1") %>%
  select(ID,ConditionOfInterest,Mean=par, StandardValues)
Parameters2$SD = Parameters$par[Parameters$parn == "p2"]
```

Finally, we perform an ANOVA over means and standard deviations of the fitted psychometric functions. We are mainly interest in the main effect of Condition of Interest, so we extract the p value for this main effect for each means and standard deviations of the psychometric function. 


```{r Staircase16, include=TRUE}
ANOVA_Mean = aov(Mean ~ as.factor(ConditionOfInterest)*StandardValues,Parameters2)
summary(ANOVA_Mean)

ANOVA_SD = aov(SD ~ as.factor(ConditionOfInterest)*StandardValues,Parameters2)
summary(ANOVA_SD)
```


**Computing the Power from Simulations for both approaches**

To simulate the power with a given set of parameters, we need to execute the above procedure sufficient times (we recommend 1000 times, although this might be too time consuming for studies with a high count of subjects and or trials; further below we give some recommendations on how to speed up the relatively slow glmer() model fitting implementation in the R package lme4), and calculate the ratio of simulations in which the test model is significantly better than the test model, given a certain false positive rate (typically 0.05). To this end, we establish functions containing the above procedure:

```{r Staircase14, include=TRUE}
SimulatePsychometricFunction_Staircase = function(ID, ConditionOfInterest, StandardValues, reps, PSE_Difference, JND_Difference, Mean_Standard, SD_Standard, SD_ResponseFunction,Mean_Variability_Between = 0.1, SD_Variability = 0.1){
  Psychometric = expand.grid(ID=ID, ConditionOfInterest=ConditionOfInterest, StandardValues=StandardValues, reps = reps)
  
  Psychometric = Psychometric %>%
    group_by(ID) %>%#
    mutate(PSE_Factor_ID = rnorm(1,1,Mean_Variability_Between),
           SD_Factor_ID = rnorm(1,1,SD_Variability))
  
  Psychometric = Psychometric %>%
    mutate(
      Mean = (StandardValues + (ConditionOfInterest==1)*StandardValues*PSE_Difference)*PSE_Factor_ID,
      SD = abs((SD_Standard + (ConditionOfInterest==1)*SD_Standard*JND_Difference)*SD_Factor_ID),
      staircase_factor = rcauchy(length(reps),1,SD_ResponseFunction), 
      Presented_TestStimulusStrength = Mean*staircase_factor,
      Difference = Presented_TestStimulusStrength - StandardValues,
      AnswerProbability = pnorm(Presented_TestStimulusStrength,Mean,SD),
      Answer = as.numeric(rbernoulli(length(AnswerProbability),AnswerProbability))
    )

  Psychometric = Psychometric %>%
    filter(abs(Difference) < 0.5*abs(Mean)) %>%
    group_by(ID,ConditionOfInterest,StandardValues,Difference) %>%
    mutate(Yes = sum(Answer==1),
           Total = length(ConditionOfInterest))
  
  Psychometric
}

Analyze_Pychometric_Accuracy_GLMM = function(Psychometric){

  TimeBeginning = Sys.time()
  
  GLMM_Accuracy = glmer(cbind(Yes, Total - Yes) ~ ConditionOfInterest + (Difference  | ID)  + (Difference  | StandardValues),
               family = binomial(link = "probit"), 
               data = Psychometric,
               nAGQ = 0,
               control = glmerControl(optimizer = "nloptwrap"))
  
  p = summary(GLMM_Accuracy)$coefficients[8]
  
  #print(TimeBeginning - Sys.time()) ###This is two show how long each iteration takes
  #print(p)
  
  p
}

Analyze_Pychometric_Precision_GLMM = function(Psychometric){

  TimeBeginning = Sys.time()
  
  GLMM_Precision = glmer(cbind(Yes, Total - Yes) ~ as.factor(ConditionOfInterest)*Difference + (Difference  | ID) + (Difference  | StandardValues), 
               family = binomial(link = "probit"), 
               data = Psychometric,
               nAGQ = 0,
               control = glmerControl(optimizer = "nloptwrap"))
  
  p = summary(GLMM_Precision)$coefficients[16]
  
  #print(TimeBeginning - Sys.time())  ###This is two show how long each iteration takes
  #print(p)
  
  p
}

GetParametersOfPsychometricFunction = function(Psychometric){
  Parameters = quickpsy(Psychometric,Difference,Answer,grouping = .(ID,ConditionOfInterest,StandardValues), bootstrap = "none")$par
  
  Parameters2 = Parameters %>%
  filter(parn == "p1") %>%
  select(ID,ConditionOfInterest,Mean=par, StandardValues)
  Parameters2$SD = Parameters$par[Parameters$parn == "p2"]
  Parameters2
}

Analyze_Pychometric_Accuracy_2Level = function(Psychometric){

  ANOVA_Mean = aov(Mean ~ as.factor(ConditionOfInterest)*StandardValues,Parameters2)
  Coefficients = summary(ANOVA_Mean)[[1]]
  Coefficients$`Pr(>F)`[1]
}
Analyze_Pychometric_Accuracy_2Level = function(Psychometric){

  ANOVA_SD = aov(SD ~ as.factor(ConditionOfInterest)*StandardValues,Parameters2)
  Coefficients = summary(ANOVA_SD)[[1]]
  Coefficients$`Pr(>F)`[1]
}

```

Now, we determine for how many subjects we want to simulate the power. Fewer than 10 subjects are usually not recommended.
```{r Staircase15, include=TRUE}
NumbersOfSubjects = c(10,12,14,16,18,20) #for how many subjects are we simulating the power?
```

Then, we execute the above method "nIterations" times for each number of subjects specified above, both for precision and accuracy from the R core package.
```{r Staircase16, include=TRUE}
PowerPerN_Precision = c()
plots_Precision = data.frame()
nIterations = 100

for (i in NumbersOfSubjects){
  ID = paste0("s",1:i)
  
  for (i in 1:nIterations){
  
    Dataframe = SimulatePsychometricFunction_Staircase(ID, ConditionOfInterest, StandardValues, reps, PSE_Difference, JND_Difference, Mean_Standard, SD_Standard, SD_ResponseFunction)
    Analyze_Pychometric_Precision_GLMM(Dataframe)
    Analyze_Pychometric_Accuracy_GLMM(Dataframe)
    Analyze_Pychometric_Precision_2Level(Dataframe)
    Analyze_Pychometric_Accuracy_2Level(Dataframe)
    
    
  }
      
  Power_Precision = c()
  nIterations = 1 #how many data frames should we simulate per number of subjects? more data frames makes for a more reliable estimate of the power
  out <- replicate(nIterations, { #this function performs the number of iterations indicated above and stores the result for each in "out"
    Analyze_Pychometric_Precision_GLMM(SimulatePsychometricFunction_Staircase(ID, ConditionOfInterest, StandardValues, reps, PSE_Difference, JND_Difference, Mean_Standard, SD_Standard, SD_ResponseFunction))})
  outout = data.frame(pvalue = out,N_Subjects = paste(i,"participants"))
  Power_Precision = mean(out < 0.05) ###Power is the times the difference between the two models is significant
  PowerPerN_Precision = c(PowerPerN_Precision,Power_Precision) ###This is a vector with the power for each n for the JNDs
  paste0("For precision & ", i, " subjects: ", Power_Precision) #outputs an estimate of the power for each n
  
  
}

ggplot(plots_Precision,aes(pvalue)) +
  geom_histogram() +
  facet_wrap(.~N_Subjects) +
  xlab("p value") +
  ylab("Density") +
  geom_vline(xintercept = 0.05)
```

```{r Staircase17, include=TRUE}
PowerPerN_Accuracy = c()
plots_Accuracy = data.frame()

for (i in NumbersOfSubjects){
  ID = paste0("s",1:i)
  
  Power_Accuracy = c()
  nIterations = 1 #how many data frames should we simulate per number of subjects? more data frames makes for a more reliable estimate of the power
  out2 <- replicate(nIterations, { #this function performs the number of iterations indicated above and stores the result for each in "out2"
    Analyze_Pychometric_Accuracy_GLMM(SimulatePsychometricFunction_Staircase(ID, ConditionOfInterest, StandardValues, reps, PSE_Difference, JND_Difference, Mean_Standard, SD_Standard, SD_ResponseFunction))})
  
  outout = data.frame(pvalue = out2,N_Subjects = paste(i,"participants"))
  plots_Accuracy = rbind(plots_Accuracy,outout)

  Power_Accuracy = mean(out2 < 0.05) ###Power is the times the difference between the two models is significant
  
  PowerPerN_Accuracy = c(PowerPerN_Accuracy,Power_Accuracy) ###This is a vector with the power for each n for the PSEs
  
  paste0("For ", i, " subjects: ", Power_Accuracy) #prints estimate of the power for each n
}

ggplot(plots_Accuracy,aes(pvalue)) +
  geom_histogram() +
  facet_wrap(.~N_Subjects) +
  xlab("p value") +
  ylab("Density") +
  geom_vline(xintercept = 0.05)
```